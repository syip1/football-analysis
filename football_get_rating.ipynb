{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d0c063",
   "metadata": {},
   "source": [
    "This code scrapes the SoFIFA website for the ratings of players from the Premier League clubs of 2020/21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2af54243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66442ba",
   "metadata": {},
   "source": [
    "Unfortunately the team names and team ids (that are used to access the webpage) need to be hard-coded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07adeaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_list = ['arsenal', 'aston-villa', 'brighton-hove-albion', 'burnley', 'chelsea', \n",
    "             'crystal-palace', 'everton', 'fulham', 'leeds-united', 'liverpool', \n",
    "             'manchester-city', 'manchester-united', 'leicester-city', 'newcastle-united', 'west-ham-united',\n",
    "             'tottenham-hotspur', 'wolverhampton-wanderers', 'southampton', 'west-bromwich-albion', 'sheffield-united']\n",
    "team_ids = [1, 2, 1808, 1796, 5, \n",
    "            1799, 7, 144, 8, 9, \n",
    "            10, 11, 95, 13, 19, \n",
    "            18, 110, 17, 109, 1794]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aca7df",
   "metadata": {},
   "source": [
    "The function below scrapes the name, position, club, and rating information from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1f69a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(soup, soup2):\n",
    "    title = soup.find('title').text.split(' ')\n",
    "    club = []\n",
    "    for i in title:\n",
    "        if (i == 'FIFA'):\n",
    "            break\n",
    "        club.append(i)\n",
    "    club = ' '.join(club)\n",
    "    #The HTML code is different for starting players, subs and reserves, hence the three separate lists.\n",
    "    starters_list = soup.find_all('tr', class_ = 'starting')\n",
    "    sub_list = soup.find_all('tr', class_ = 'sub')\n",
    "    res_list = soup.find_all('tr', class_ = 'res')\n",
    "    name_arr = []\n",
    "    pos_arr = []\n",
    "    rat_arr = []\n",
    "    #It seemed easier to deal with the attributes separately (since they were under different HTML tags).\n",
    "    for i in range(len(starters_list)):\n",
    "        #Note we convert the name to English characters since the Premier League names are mostly in English characters\n",
    "        name_arr.append(unidecode(starters_list[i].find('div', class_ = 'ellipsis').text))\n",
    "        pos_arr.append(starters_list[i].find('span', class_ = 'pos').text)\n",
    "        rat_arr.append(starters_list[i].find('span', class_ = 'bp3-tag').text)\n",
    "    for i in range(len(sub_list)):\n",
    "        name_arr.append(unidecode(sub_list[i].find('div', class_ = 'ellipsis').text))\n",
    "        pos_arr.append(sub_list[i].find('span', class_ = 'pos').text)\n",
    "        rat_arr.append(sub_list[i].find('span', class_ = 'bp3-tag').text)\n",
    "    for i in range(len(res_list)):\n",
    "        name_arr.append(unidecode(res_list[i].find('div', class_ = 'ellipsis').text))\n",
    "        pos_arr.append(res_list[i].find('span', class_ = 'pos').text)\n",
    "        rat_arr.append(res_list[i].find('span', class_ = 'bp3-tag').text)\n",
    "    \n",
    "    #Now look at new players from January\n",
    "    starters_list_2 = soup2.find_all('tr', class_ = 'starting')\n",
    "    sub_list_2 = soup2.find_all('tr', class_ = 'sub')\n",
    "    res_list_2 = soup2.find_all('tr', class_ = 'res')\n",
    "    for i in range(len(starters_list_2)):\n",
    "        name = unidecode(starters_list_2[i].find('div', class_ = 'ellipsis').text)\n",
    "        if (name not in name_arr):\n",
    "            name_arr.append(name)\n",
    "            pos_arr.append(starters_list_2[i].find('span', class_ = 'pos').text)\n",
    "            rat_arr.append(starters_list_2[i].find('span', class_ = 'bp3-tag').text)\n",
    "    for i in range(len(sub_list_2)):\n",
    "        name = unidecode(sub_list_2[i].find('div', class_ = 'ellipsis').text)\n",
    "        if (name not in name_arr):\n",
    "            name_arr.append(name)\n",
    "            pos_arr.append(sub_list_2[i].find('span', class_ = 'pos').text)\n",
    "            rat_arr.append(sub_list_2[i].find('span', class_ = 'bp3-tag').text)\n",
    "    for i in range(len(res_list_2)):\n",
    "        name = unidecode(res_list_2[i].find('div', class_ = 'ellipsis').text)\n",
    "        if (name not in name_arr):\n",
    "            name_arr.append(name)\n",
    "            pos_arr.append(res_list_2[i].find('span', class_ = 'pos').text)\n",
    "            rat_arr.append(res_list_2[i].find('span', class_ = 'bp3-tag').text)\n",
    "    \n",
    "    assert(len(name_arr) == len(pos_arr))\n",
    "    assert(len(name_arr) == len(rat_arr))\n",
    "    club_arr = [club] * (len(name_arr))\n",
    "    df_players = pd.DataFrame()\n",
    "    df_players['Name'] = name_arr\n",
    "    df_players['Club'] = club_arr\n",
    "    df_players['Position'] = pos_arr\n",
    "    df_players['Rating'] = rat_arr\n",
    "    return df_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d9fc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_arr = []\n",
    "for i in range(len(team_list)):\n",
    "    #First url is from September\n",
    "    url1 = \"https://sofifa.com/team/\"+str(team_ids[i])+\"/\"+team_list[i]+\"/?r=210002&set=true\"\n",
    "    #Second url is from January\n",
    "    url2 = \"https://sofifa.com/team/\"+str(team_ids[i])+\"/\"+team_list[i]+\"/?r=210026&set=true\"\n",
    "    page1 = requests.get(url1)\n",
    "    page2 = requests.get(url2)\n",
    "    soup1 = BeautifulSoup(page1.content, 'html.parser')\n",
    "    soup2 = BeautifulSoup(page2.content, 'html.parser')\n",
    "    players_df = get_df(soup1, soup2)\n",
    "    dfs_arr.append(players_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff10b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players_df = pd.concat(dfs_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaf767",
   "metadata": {},
   "source": [
    "We save the dataframe in a csv file so that we don't have to do the scraping again next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7abc0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players_df.to_csv('ratings.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d19aceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 4)\n"
     ]
    }
   ],
   "source": [
    "print(all_players_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1e0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
